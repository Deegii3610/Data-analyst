{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with simple text data\n",
    "data = {\n",
    "    'content': [\n",
    "        \"apple banana cherry durian elderberry fig.\",\n",
    "        \"apple banana durian cherry elderberry cherry.\",\n",
    "        \"cherry grape honeydew ice apple cherry.\",\n",
    "        \"apple banana cherry durian grape ice\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "docs = pd.DataFrame(data)['content'][:2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a4 b3 c6 d3 e2 f1 g2 h1 i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple' 'banana' 'cherry' 'durian' 'elderberry' 'fig']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1]\n",
      " [1 1 2 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(1, 1):\n",
      "Feature Names: ['apple' 'banana' 'cherry' 'durian' 'elderberry' 'fig']\n",
      "Tokens:\n",
      "[[1 1 1 1 1 1]\n",
      " [1 1 2 1 1 0]]\n",
      "Number of tokens: 6\n",
      "==================================================\n",
      "ngram_range=(1, 2):\n",
      "Feature Names: ['apple' 'apple banana' 'banana' 'banana cherry' 'banana durian' 'cherry'\n",
      " 'cherry durian' 'cherry elderberry' 'durian' 'durian cherry'\n",
      " 'durian elderberry' 'elderberry' 'elderberry cherry' 'elderberry fig'\n",
      " 'fig']\n",
      "Tokens:\n",
      "[[1 1 1 1 0 1 1 0 1 0 1 1 0 1 1]\n",
      " [1 1 1 0 1 2 0 1 1 1 0 1 1 0 0]]\n",
      "Number of tokens: 15\n",
      "==================================================\n",
      "ngram_range=(2, 2):\n",
      "Feature Names: ['apple banana' 'banana cherry' 'banana durian' 'cherry durian'\n",
      " 'cherry elderberry' 'durian cherry' 'durian elderberry'\n",
      " 'elderberry cherry' 'elderberry fig']\n",
      "Tokens:\n",
      "[[1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0]]\n",
      "Number of tokens: 9\n",
      "==================================================\n",
      "ngram_range=(2, 3):\n",
      "Feature Names: ['apple banana' 'apple banana cherry' 'apple banana durian'\n",
      " 'banana cherry' 'banana cherry durian' 'banana durian'\n",
      " 'banana durian cherry' 'cherry durian' 'cherry durian elderberry'\n",
      " 'cherry elderberry' 'cherry elderberry cherry' 'durian cherry'\n",
      " 'durian cherry elderberry' 'durian elderberry' 'durian elderberry fig'\n",
      " 'elderberry cherry' 'elderberry fig']\n",
      "Tokens:\n",
      "[[1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1]\n",
      " [1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0]]\n",
      "Number of tokens: 17\n",
      "==================================================\n",
      "ngram_range=(3, 3):\n",
      "Feature Names: ['apple banana cherry' 'apple banana durian' 'banana cherry durian'\n",
      " 'banana durian cherry' 'cherry durian elderberry'\n",
      " 'cherry elderberry cherry' 'durian cherry elderberry'\n",
      " 'durian elderberry fig']\n",
      "Tokens:\n",
      "[[1 0 1 0 1 0 0 1]\n",
      " [0 1 0 1 0 1 1 0]]\n",
      "Number of tokens: 8\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for ngram_range in [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3)]:\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    print(f\"ngram_range={ngram_range}:\")\n",
    "    print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "    print(\"Tokens:\")\n",
    "    print(X.toarray())\n",
    "    print(\"Number of tokens:\", X.toarray().shape[1])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with min_df=1: 6\n",
      "Number of tokens with min_df=2: 5\n",
      "Number of tokens with max_df=1: 1\n",
      "Number of tokens with max_df=2: 6\n"
     ]
    }
   ],
   "source": [
    "vectorizer_min_df_1 = CountVectorizer(min_df=1)\n",
    "X_min_df_1 = vectorizer_min_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with min_df=2\n",
    "vectorizer_min_df_2 = CountVectorizer(min_df=2)\n",
    "X_min_df_2 = vectorizer_min_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with min_df=1\n",
    "num_tokens_min_df_1 = X_min_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with min_df=2\n",
    "num_tokens_min_df_2 = X_min_df_2.shape[1]\n",
    "\n",
    "# Initialize CountVectorizer with max_df=1\n",
    "vectorizer_max_df_1 = CountVectorizer(max_df=1)\n",
    "X_max_df_1 = vectorizer_max_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with max_df=2\n",
    "vectorizer_max_df_2 = CountVectorizer(max_df=2)\n",
    "X_max_df_2 = vectorizer_max_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with max_df=1\n",
    "num_tokens_max_df_1 = X_max_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with max_df=2\n",
    "num_tokens_max_df_2 = X_max_df_2.shape[1]\n",
    "\n",
    "print(f\"Number of tokens with min_df=1: {num_tokens_min_df_1}\")\n",
    "print(f\"Number of tokens with min_df=2: {num_tokens_min_df_2}\")\n",
    "print(f\"Number of tokens with max_df=1: {num_tokens_max_df_1}\")\n",
    "print(f\"Number of tokens with max_df=2: {num_tokens_max_df_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple' 'banana' 'cherry' 'durian' 'elderberry']\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 2. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None,min_df=2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Token 'apple' has the largest TF-IDF value of 1.00\n",
      "Document 2: Token 'cherry' has the largest TF-IDF value of 2.00\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Find the index of the token with the highest TF-IDF value in each document\n",
    "max_tfidf_indices = tfidf_matrix.argmax(axis=1)\n",
    "\n",
    "# Get the token with the largest TF-IDF value in each document\n",
    "for doc_index, max_index in enumerate(max_tfidf_indices):\n",
    "    token = feature_names[max_index]\n",
    "    tfidf_value = tfidf_matrix[doc_index, max_index]\n",
    "    print(f\"Document {doc_index + 1}: Token '{token}' has the largest TF-IDF value of {tfidf_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import numpy as np\\ndocs = pd.DataFrame(data)['content'][:2]\\nvectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.2)\\nX = vectorizer.fit_transform(docs)\\nindices = np.arange(docs.size)\\nfrom sklearn.model_selection import train_test_split\\ntrain, test = train_test_split(indices, test_size=0.2)\\nfrom sklearn.metrics.pairwise import cosine_distances\\nfrom sklearn.neighbors import NearestNeighbors\\nnbrs = NearestNeighbors(n_neighbors=3,metric=cosine_distances).fit(X[train])\\ntest=[test[0]]\\nfound = nbrs.kneighbors(X[test], return_distance=False)\\ntest_i=0\\nprint('text:\\n%.300s'%docs[test[test_i]])\\nfor i in found[0]:\\n    print('match %d:\\n%.300s'%(i,docs[train[i]]))\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "docs = pd.DataFrame(data)['content'][:2]\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "indices = np.arange(docs.size)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(indices, test_size=0.2)\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=3,metric=cosine_distances).fit(X[train])\n",
    "test=[test[0]]\n",
    "found = nbrs.kneighbors(X[test], return_distance=False)\n",
    "test_i=0\n",
    "print('text:\\n%.300s'%docs[test[test_i]])\n",
    "for i in found[0]:\n",
    "    print('match %d:\\n%.300s'%(i,docs[train[i]]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "apple banana durian cherry elderberry cherry.\n",
      "match 0:\n",
      "apple banana cherry durian elderberry fig.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(data)['content'][:2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "indices = np.arange(docs.size)\n",
    "train, test = train_test_split(indices, test_size=0.2)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=1, metric=cosine_distances).fit(X[train])\n",
    "test = [test[0]]\n",
    "found = nbrs.kneighbors(X[test], return_distance=False)\n",
    "\n",
    "test_i = 0\n",
    "print('text:\\n%.300s' % docs[test[test_i]])\n",
    "for i in found[0]:\n",
    "    print('match %d:\\n%.300s' % (i, docs[train[i]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
