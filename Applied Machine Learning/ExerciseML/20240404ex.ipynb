{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "docs = pd.read_csv(\"spacenews.csv\")['content'][:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '11' '12' '13' '18' '20' '2017' '2019' '2025' '2026' '21' '360'\n",
      " '368' '58' '60' 'ability' 'able' 'about' 'achieve' 'achieving' 'acquired'\n",
      " 'acquisition' 'added' 'additional' 'adjacent' 'advanced' 'advisor'\n",
      " 'aerospace' 'affecting' 'after' 'against' 'ago' 'agreement' 'all'\n",
      " 'allows' 'also' 'although' 'altogether' 'an' 'analytics' 'and'\n",
      " 'announced' 'announcement' 'another' 'any' 'applications' 'are' 'areas'\n",
      " 'argued' 'around' 'artificial' 'as' 'assets' 'at' 'aug' 'available'\n",
      " 'aviation' 'away' 'balance' 'band' 'based' 'be' 'been' 'being' 'believe'\n",
      " 'best' 'better' 'between' 'bid' 'big' 'biggest' 'billion' 'bit'\n",
      " 'blackrock' 'block' 'board' 'boeing' 'both' 'brings' 'broadband'\n",
      " 'broader' 'budget' 'built' 'bullet' 'burst' 'business' 'businesses' 'but'\n",
      " 'buy' 'by' 'cabling' 'call' 'can' 'capabilities' 'capability' 'capacity'\n",
      " 'capital' 'case' 'cash' 'caught' 'ceo' 'ceos' 'certain' 'certainly'\n",
      " 'challenges' 'change' 'changes' 'chief' 'choice' 'civil' 'claim'\n",
      " 'clarity' 'clearing' 'close' 'closed' 'closely' 'clusters' 'collar'\n",
      " 'come' 'comes' 'coming' 'commercial' 'commission' 'commitment'\n",
      " 'companies' 'company' 'compelling' 'compete' 'competition' 'competitor'\n",
      " 'complicate' 'concentration' 'concern' 'concerned' 'conditions'\n",
      " 'conference' 'confidence' 'conflict' 'connected' 'connectivity'\n",
      " 'conservative' 'considering' 'consistent' 'consolidation' 'constellation'\n",
      " 'continue' 'contracted' 'control' 'could' 'course' 'cruise' 'current'\n",
      " 'currently' 'customers' 'data' 'date' 'day' 'deal' 'decide' 'decided'\n",
      " 'decision' 'declined' 'defense' 'deforestation' 'delayed' 'demand'\n",
      " 'departure' 'depend' 'deploy' 'deploying' 'design' 'designs' 'details'\n",
      " 'detract' 'development' 'did' 'didn' 'different' 'difficult' 'directly'\n",
      " 'disclosed' 'discuss' 'discussed' 'disrupted' 'diverging' 'do' 'doesn'\n",
      " 'doing' 'don' 'doubled' 'down' 'drive' 'drowning' 'drs' 'during'\n",
      " 'earnings' 'earth' 'economics' 'edited' 'eight' 'electrical' 'electron'\n",
      " 'enable' 'end' 'endorsing' 'engaged' 'environment' 'erosion' 'especially'\n",
      " 'euroconsult' 'european' 'eutelsat' 'even' 'evolving' 'example' 'execute'\n",
      " 'executive' 'exist' 'existing' 'expect' 'expected' 'expects' 'experience'\n",
      " 'experiences' 'explores' 'extent' 'facility' 'falcon' 'far' 'fast'\n",
      " 'feature' 'felt' 'fence' 'fifth' 'finalize' 'financial' 'financials'\n",
      " 'firm' 'first' 'firstly' 'fishing' 'five' 'fixed' 'flexibility' 'flight'\n",
      " 'flow' 'flows' 'fluid' 'focus' 'focused' 'followed' 'following' 'for'\n",
      " 'forcing' 'forecasts' 'foreseeable' 'four' 'frequency' 'from' 'full'\n",
      " 'fundamental' 'funding' 'future' 'general' 'generation' 'geo'\n",
      " 'geographies' 'geography' 'geolocation' 'geostationary' 'get' 'getting'\n",
      " 'give' 'given' 'gives' 'giving' 'glitch' 'go' 'goal' 'goals' 'going'\n",
      " 'good' 'got' 'government' 'governments' 'gps' 'grade' 'great' 'ground'\n",
      " 'growing' 'growth' 'guaranteed' 'had' 'half' 'handles' 'hands' 'happen'\n",
      " 'happened' 'happening' 'hard' 'hardware' 'harsh' 'has' 'have' 'having'\n",
      " 'hawkeye' 'he' 'heck' 'helm' 'help' 'here' 'highlighted' 'him' 'his'\n",
      " 'hope' 'horizon' 'hotspots' 'how' 'if' 'illegal' 'impact' 'importance'\n",
      " 'in' 'included' 'including' 'increasing' 'industry' 'inflection'\n",
      " 'infrastructure' 'initial' 'initiatives' 'inmarsat' 'institute'\n",
      " 'intelligence' 'intelsat' 'interest' 'interesting' 'interference'\n",
      " 'internal' 'interview' 'into' 'invasion' 'invest' 'investing'\n",
      " 'investment' 'investor' 'ipo' 'iris²' 'is' 'issue' 'it' 'its' 'john'\n",
      " 'joined' 'joint' 'jointly' 'july' 'june' 'just' 'ka' 'keeps' 'key' 'know'\n",
      " 'ku' 'kuiper' 'lab' 'laboratory' 'larger' 'largest' 'last' 'later'\n",
      " 'launch' 'launching' 'launchpad' 'layer' 'lead' 'leadership' 'learn'\n",
      " 'learned' 'learning' 'leaving' 'led' 'left' 'length' 'level' 'leverage'\n",
      " 'life' 'lifetime' 'lightspeed' 'like' 'likely' 'little' 'live' 'logic'\n",
      " 'long' 'look' 'looking' 'losing' 'lot' 'luxembourg' 'machine' 'main'\n",
      " 'make' 'making' 'managed' 'management' 'many' 'market' 'markets' 'masses'\n",
      " 'may' 'mean' 'mediterranean' 'medium' 'meet' 'meo' 'milestones' 'million'\n",
      " 'mindful' 'mitigate' 'mix' 'model' 'modules' 'moment' 'money' 'months'\n",
      " 'more' 'move' 'mpower' 'much' 'multi' 'myself' 'nature' 'necessarily'\n",
      " 'need' 'needed' 'needs' 'negotiations' 'neighbor' 'network' 'networks'\n",
      " 'new' 'news' 'next' 'no' 'nobody' 'northern' 'not' 'now' 'o3b'\n",
      " 'objective' 'observed' 'october' 'of' 'off' 'offer' 'offering' 'officer'\n",
      " 'offs' 'often' 'on' 'once' 'one' 'oneweb' 'open' 'operate' 'operating'\n",
      " 'operational' 'operations' 'operator' 'opportunities' 'options' 'or'\n",
      " 'orbit' 'other' 'otherwise' 'our' 'ours' 'out' 'outlined' 'outstripped'\n",
      " 'over' 'own' 'pace' 'packed' 'paris' 'part' 'partly' 'partner'\n",
      " 'partnered' 'partners' 'partnerships' 'past' 'path' 'pay' 'payload'\n",
      " 'pays' 'performance' 'performs' 'period' 'phenomena' 'phenomenon' 'pinto'\n",
      " 'place' 'plan' 'plans' 'pleased' 'point' 'points' 'position' 'possible'\n",
      " 'potential' 'potentially' 'power' 'premium' 'presentation' 'president'\n",
      " 'press' 'prevent' 'prevents' 'previous' 'previously' 'price' 'pricing'\n",
      " 'private' 'probably' 'proceeds' 'produce' 'products' 'profitability'\n",
      " 'profitable' 'project' 'promote' 'promoted' 'proud' 'prove' 'provide'\n",
      " 'provided' 'provider' 'public' 'put' 'quick' 'quickly' 'quite' 'radio'\n",
      " 'rainhart' 'raise' 'raised' 're' 'reach' 'reached' 'reaching' 'ready'\n",
      " 'realizing' 'really' 'receipt' 'recent' 'recently' 'reconnaissance'\n",
      " 'record' 'regionally' 'release' 'remain' 'remains' 'remote' 'replace'\n",
      " 'replaced' 'replacements' 'replicate' 'requirements' 'requisite'\n",
      " 'resolved' 'resolving' 'responsibilities' 'result' 'retiring' 'reusable'\n",
      " 'revenue' 'right' 'rob' 'rocket' 'role' 'round' 'running' 'rushing'\n",
      " 'russia' 'ruy' 'said' 'sake' 'same' 'satcoms' 'satellite' 'satellites'\n",
      " 'say' 'saying' 'says' 'scale' 'scheduled' 'seamlessly' 'second'\n",
      " 'security' 'see' 'seen' 'segment' 'segments' 'selective' 'sell'\n",
      " 'sensible' 'sensing' 'sept' 'september' 'serafini' 'series' 'serious'\n",
      " 'service' 'services' 'ses' 'set' 'sheet' 'shelf' 'shielding' 'shift'\n",
      " 'ship' 'ships' 'short' 'should' 'side' 'sidelines' 'sign' 'signals'\n",
      " 'silver' 'since' 'single' 'six' 'sixth' 'slated' 'sleep' 'slot' 'small'\n",
      " 'smaller' 'so' 'software' 'some' 'something' 'sometimes' 'soon' 'source'\n",
      " 'sources' 'space' 'spacenews' 'specific' 'spectrum' 'sporadically' 'spot'\n",
      " 'starlink' 'state' 'staying' 'steve' 'still' 'stock' 'strategic'\n",
      " 'strategy' 'strong' 'stronghold' 'studies' 'success' 'such' 'sudden'\n",
      " 'summer' 'supply' 'support' 'surprise' 'surprised' 'surveillance'\n",
      " 'systems' 'tackling' 'tactical' 'take' 'talk' 'target' 'team' 'technical'\n",
      " 'technology' 'telesat' 'tenets' 'term' 'terms' 'than' 'that' 'the'\n",
      " 'their' 'them' 'then' 'there' 'they' 'thing' 'things' 'think' 'thinking'\n",
      " 'this' 'those' 'though' 'three' 'throwing' 'time' 'timeline' 'timing'\n",
      " 'to' 'together' 'tomorrow' 'tools' 'toronto' 'towards' 'tracking' 'tried'\n",
      " 'trip' 'tripping' 'turns' 'two' 'ukraine' 'ultimately' 'under'\n",
      " 'understand' 'understanding' 'unfortunate' 'unique' 'unit' 'university'\n",
      " 'up' 'upcoming' 'updated' 'updates' 'upgraded' 'us' 'users' 'valuable'\n",
      " 've' 'verge' 'versa' 'very' 'viability' 'viasat' 'vice' 'views'\n",
      " 'virginia' 'want' 'was' 'wasn' 'way' 'we' 'week' 'weeks' 'well' 'were'\n",
      " 'what' 'when' 'where' 'whether' 'which' 'who' 'will' 'willing' 'window'\n",
      " 'wish' 'with' 'without' 'won' 'words' 'work' 'working' 'world' 'worlds'\n",
      " 'would' 'year' 'years' 'yet' 'you' 'your' 'zealand']\n",
      "[[ 0  0  2 ...  2  0  1]\n",
      " [ 2  1  1 ... 14  2  0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(1, 2):\n",
      "Feature Names: ['10' '10 years' '11' ... 'your take' 'zealand' 'zealand the']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 1 1]\n",
      " [2 2 1 ... 1 0 0]]\n",
      "Number of tokens: 2819\n",
      "==================================================\n",
      "ngram_range=(2, 2):\n",
      "Feature Names: ['10 years' '11 ka' '12 announcement' ... 'your leadership' 'your take'\n",
      " 'zealand the']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [2 1 1 ... 1 1 0]]\n",
      "Number of tokens: 2040\n",
      "==================================================\n",
      "ngram_range=(2, 3):\n",
      "Feature Names: ['10 years' '10 years but' '10 years of' ... 'your take on' 'zealand the'\n",
      " 'zealand the company']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 1 1]\n",
      " [2 1 1 ... 1 0 0]]\n",
      "Number of tokens: 4420\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer with different n-gram parameters\n",
    "for ngram_range in [(1, 2), (2, 2), (2, 3)]:\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    print(f\"ngram_range={ngram_range}:\")\n",
    "    print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "    print(\"Tokens:\")\n",
    "    print(X.toarray())\n",
    "    print(\"Number of tokens:\", X.toarray().shape[1])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with min_df=1: 779\n",
      "Number of tokens with min_df=2: 151\n",
      "Number of tokens with max_df=1: 628\n",
      "Number of tokens with max_df=2: 779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize CountVectorizer with min_df=1\n",
    "vectorizer_min_df_1 = CountVectorizer(min_df=1)\n",
    "X_min_df_1 = vectorizer_min_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with min_df=2\n",
    "vectorizer_min_df_2 = CountVectorizer(min_df=2)\n",
    "X_min_df_2 = vectorizer_min_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with min_df=1\n",
    "num_tokens_min_df_1 = X_min_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with min_df=2\n",
    "num_tokens_min_df_2 = X_min_df_2.shape[1]\n",
    "\n",
    "# Initialize CountVectorizer with max_df=1\n",
    "vectorizer_max_df_1 = CountVectorizer(max_df=1)\n",
    "X_max_df_1 = vectorizer_max_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with max_df=2\n",
    "vectorizer_max_df_2 = CountVectorizer(max_df=2)\n",
    "X_max_df_2 = vectorizer_max_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with max_df=1\n",
    "num_tokens_max_df_1 = X_max_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with max_df=2\n",
    "num_tokens_max_df_2 = X_max_df_2.shape[1]\n",
    "\n",
    "print(f\"Number of tokens with min_df=1: {num_tokens_min_df_1}\")\n",
    "print(f\"Number of tokens with min_df=2: {num_tokens_min_df_2}\")\n",
    "print(f\"Number of tokens with max_df=1: {num_tokens_max_df_1}\")\n",
    "print(f\"Number of tokens with max_df=2: {num_tokens_max_df_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12' '20' 'about' 'ago' 'all' 'also' 'although' 'an' 'and' 'another'\n",
      " 'applications' 'are' 'as' 'at' 'be' 'been' 'being' 'both' 'business'\n",
      " 'but' 'by' 'can' 'capabilities' 'capital' 'chief' 'company' 'conference'\n",
      " 'conservative' 'continue' 'could' 'customers' 'data' 'demand' 'didn'\n",
      " 'different' 'do' 'doesn' 'felt' 'for' 'frequency' 'from' 'future' 'get'\n",
      " 'go' 'going' 'good' 'great' 'has' 'have' 'he' 'him' 'if' 'in' 'initial'\n",
      " 'interview' 'into' 'is' 'issue' 'it' 'its' 'just' 'last' 'later' 'launch'\n",
      " 'leverage' 'like' 'lot' 'make' 'market' 'may' 'mix' 'money' 'months'\n",
      " 'more' 'move' 'much' 'need' 'next' 'not' 'now' 'of' 'officer' 'on' 'one'\n",
      " 'or' 'orbit' 'other' 'our' 'out' 'over' 'paris' 'plan' 'plans' 'point'\n",
      " 'position' 'possible' 'potentially' 'products' 'put' 're' 'reached'\n",
      " 'right' 'rushing' 'said' 'satellite' 'satellites' 'service' 'since'\n",
      " 'smaller' 'so' 'some' 'still' 'such' 'tenets' 'term' 'than' 'that' 'the'\n",
      " 'their' 'them' 'then' 'there' 'they' 'think' 'this' 'those' 'three'\n",
      " 'throwing' 'time' 'to' 'together' 'two' 'understanding' 'up' 'us' 'very'\n",
      " 'was' 'we' 'week' 'well' 'what' 'where' 'which' 'will' 'with' 'work'\n",
      " 'world' 'would' 'year' 'years' 'you']\n",
      "[[ 2.  1.  1.  2.  1.  2.  1.  6. 22.  1.  2.  3. 13.  3.  3.  1.  1.  3.\n",
      "   2.  2.  3.  5.  3.  1.  3. 14.  1.  1.  1.  1.  3.  4.  1.  1.  1.  3.\n",
      "   1.  1.  9.  2.  3.  1.  1.  1.  3.  1.  1.  7.  3. 13.  1.  1. 15.  1.\n",
      "   1.  3.  7.  1.  6.  2.  1.  1.  1.  2.  1.  3.  1.  1.  2.  1.  1.  2.\n",
      "   1.  2.  1.  1.  2.  1.  2.  1. 19.  1.  7.  4.  3.  1.  1.  3.  1.  1.\n",
      "   1.  1.  2.  2.  2.  1.  1.  1.  1.  3.  1.  2.  1. 15.  2.  5.  1.  2.\n",
      "   1.  1.  1.  1.  2.  1.  1.  1. 13. 47.  1.  1.  1.  1.  1.  1.  1.  3.\n",
      "   3.  1.  2. 25.  1.  3.  1.  1.  2.  2.  2.  9.  1.  3.  1.  1.  4.  5.\n",
      "   4.  5.  1.  2.  3.  3.  2.]\n",
      " [ 1.  1.  3.  1.  2.  7.  1.  5. 53.  1.  1. 15. 13.  8. 10.  9.  4.  2.\n",
      "   2. 14.  4.  6.  1.  1.  1.  4.  1.  1.  1.  3.  7.  1.  3.  2.  1.  4.\n",
      "   5.  1. 25.  1. 11.  1.  2.  3.  6.  7.  1.  6. 29.  3.  2.  4. 47.  2.\n",
      "   1.  1. 23.  2. 21.  4.  4.  1.  1.  5.  1.  4.  2.  2.  6.  2.  1.  2.\n",
      "   4.  6.  1.  1.  3.  2.  9.  2. 35.  1. 20.  1.  6.  5.  2. 15.  2.  4.\n",
      "   1.  1.  1.  1.  3.  1.  1.  1.  2.  9.  1.  2.  1.  2.  2. 10.  2.  1.\n",
      "   1.  5.  4.  2.  1.  1.  1.  3. 31. 85.  3.  6.  1. 11.  8.  1.  8.  2.\n",
      "   1.  1. 10. 57.  1.  6.  2.  3.  7.  6. 10. 56.  1.  1. 10.  3.  4. 10.\n",
      "  21.  1.  1.  2.  1.  7. 14.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None,min_df=2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Token 'the' has the largest TF-IDF value of 47.00\n",
      "Document 2: Token 'the' has the largest TF-IDF value of 85.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Find the index of the token with the highest TF-IDF value in each document\n",
    "max_tfidf_indices = tfidf_matrix.argmax(axis=1)\n",
    "\n",
    "# Get the token with the largest TF-IDF value in each document\n",
    "for doc_index, max_index in enumerate(max_tfidf_indices):\n",
    "    token = feature_names[max_index]\n",
    "    tfidf_value = tfidf_matrix[doc_index, max_index]\n",
    "    print(f\"Document {doc_index + 1}: Token '{token}' has the largest TF-IDF value of {tfidf_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "WASHINGTON — As India prepares to launch its second lunar lander mission, the fate of a second Israeli lander is in doubt after the organization developing it lost a major source of funding. India’s Chandrayaan-3 spacecraft is scheduled to launch July 14 on a Geosynchronous Satellite Launch Vehicle \n",
      "match 286:\n",
      "HELSINKI — India will make its second moon landing attempt in 18 days’ time after its Chandrayaan-3 spacecraft arrived in lunar orbit Saturday. Chandrayaan-3 began a roughly 30-minute burn around 9:30 a.m. Eastern, seeing the spacecraft enter an elliptical lunar orbit, the Indian Space Research Orga\n",
      "match 700:\n",
      "HELSINKI — India’s Chandrayaan-3 lander successfully touched down on the moon Wednesday, making the country only the fourth to achieve the feat. The Chandrayaan-3 mission lander touched down in the vicinity of the lunar South Pole region at 8:32 a.m. Eastern (1232 UTC) Aug. 23 after a 19-minute powe\n",
      "match 450:\n",
      "WASHINGTON — The technical success of India’s Chandrayaan-3 lunar lander mission could help not just India’s space program but also the country’s standing on the global stage, experts argue. The Indian space agency ISRO  put the Vikram lander into sleep mode late Sept. 3 , shortly before nightfall a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "docs = pd.read_csv(\"spacenews.csv\")['content']\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "indices = np.arange(docs.size)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(indices, test_size=0.2)\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=3,metric=cosine_distances).fit(X[train])\n",
    "test=[test[0]]\n",
    "found = nbrs.kneighbors(X[test], return_distance=False)\n",
    "test_i=0\n",
    "print('text:\\n%.300s'%docs[test[test_i]])\n",
    "for i in found[0]:\n",
    "    print('match %d:\\n%.300s'%(i,docs[train[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
