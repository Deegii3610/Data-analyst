{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "docs = pd.read_csv(\"spacenews.csv\")['content'][:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '11' '12' '13' '18' '20' '2017' '2019' '2025' '2026' '21' '360'\n",
      " '368' '58' '60' 'ability' 'able' 'about' 'achieve' 'achieving' 'acquired'\n",
      " 'acquisition' 'added' 'additional' 'adjacent' 'advanced' 'advisor'\n",
      " 'aerospace' 'affecting' 'after' 'against' 'ago' 'agreement' 'all'\n",
      " 'allows' 'also' 'although' 'altogether' 'an' 'analytics' 'and'\n",
      " 'announced' 'announcement' 'another' 'any' 'applications' 'are' 'areas'\n",
      " 'argued' 'around' 'artificial' 'as' 'assets' 'at' 'aug' 'available'\n",
      " 'aviation' 'away' 'balance' 'band' 'based' 'be' 'been' 'being' 'believe'\n",
      " 'best' 'better' 'between' 'bid' 'big' 'biggest' 'billion' 'bit'\n",
      " 'blackrock' 'block' 'board' 'boeing' 'both' 'brings' 'broadband'\n",
      " 'broader' 'budget' 'built' 'bullet' 'burst' 'business' 'businesses' 'but'\n",
      " 'buy' 'by' 'cabling' 'call' 'can' 'capabilities' 'capability' 'capacity'\n",
      " 'capital' 'case' 'cash' 'caught' 'ceo' 'ceos' 'certain' 'certainly'\n",
      " 'challenges' 'change' 'changes' 'chief' 'choice' 'civil' 'claim'\n",
      " 'clarity' 'clearing' 'close' 'closed' 'closely' 'clusters' 'collar'\n",
      " 'come' 'comes' 'coming' 'commercial' 'commission' 'commitment'\n",
      " 'companies' 'company' 'compelling' 'compete' 'competition' 'competitor'\n",
      " 'complicate' 'concentration' 'concern' 'concerned' 'conditions'\n",
      " 'conference' 'confidence' 'conflict' 'connected' 'connectivity'\n",
      " 'conservative' 'considering' 'consistent' 'consolidation' 'constellation'\n",
      " 'continue' 'contracted' 'control' 'could' 'course' 'cruise' 'current'\n",
      " 'currently' 'customers' 'data' 'date' 'day' 'deal' 'decide' 'decided'\n",
      " 'decision' 'declined' 'defense' 'deforestation' 'delayed' 'demand'\n",
      " 'departure' 'depend' 'deploy' 'deploying' 'design' 'designs' 'details'\n",
      " 'detract' 'development' 'did' 'didn' 'different' 'difficult' 'directly'\n",
      " 'disclosed' 'discuss' 'discussed' 'disrupted' 'diverging' 'do' 'doesn'\n",
      " 'doing' 'don' 'doubled' 'down' 'drive' 'drowning' 'drs' 'during'\n",
      " 'earnings' 'earth' 'economics' 'edited' 'eight' 'electrical' 'electron'\n",
      " 'enable' 'end' 'endorsing' 'engaged' 'environment' 'erosion' 'especially'\n",
      " 'euroconsult' 'european' 'eutelsat' 'even' 'evolving' 'example' 'execute'\n",
      " 'executive' 'exist' 'existing' 'expect' 'expected' 'expects' 'experience'\n",
      " 'experiences' 'explores' 'extent' 'facility' 'falcon' 'far' 'fast'\n",
      " 'feature' 'felt' 'fence' 'fifth' 'finalize' 'financial' 'financials'\n",
      " 'firm' 'first' 'firstly' 'fishing' 'five' 'fixed' 'flexibility' 'flight'\n",
      " 'flow' 'flows' 'fluid' 'focus' 'focused' 'followed' 'following' 'for'\n",
      " 'forcing' 'forecasts' 'foreseeable' 'four' 'frequency' 'from' 'full'\n",
      " 'fundamental' 'funding' 'future' 'general' 'generation' 'geo'\n",
      " 'geographies' 'geography' 'geolocation' 'geostationary' 'get' 'getting'\n",
      " 'give' 'given' 'gives' 'giving' 'glitch' 'go' 'goal' 'goals' 'going'\n",
      " 'good' 'got' 'government' 'governments' 'gps' 'grade' 'great' 'ground'\n",
      " 'growing' 'growth' 'guaranteed' 'had' 'half' 'handles' 'hands' 'happen'\n",
      " 'happened' 'happening' 'hard' 'hardware' 'harsh' 'has' 'have' 'having'\n",
      " 'hawkeye' 'he' 'heck' 'helm' 'help' 'here' 'highlighted' 'him' 'his'\n",
      " 'hope' 'horizon' 'hotspots' 'how' 'if' 'illegal' 'impact' 'importance'\n",
      " 'in' 'included' 'including' 'increasing' 'industry' 'inflection'\n",
      " 'infrastructure' 'initial' 'initiatives' 'inmarsat' 'institute'\n",
      " 'intelligence' 'intelsat' 'interest' 'interesting' 'interference'\n",
      " 'internal' 'interview' 'into' 'invasion' 'invest' 'investing'\n",
      " 'investment' 'investor' 'ipo' 'irisÂ²' 'is' 'issue' 'it' 'its' 'john'\n",
      " 'joined' 'joint' 'jointly' 'july' 'june' 'just' 'ka' 'keeps' 'key' 'know'\n",
      " 'ku' 'kuiper' 'lab' 'laboratory' 'larger' 'largest' 'last' 'later'\n",
      " 'launch' 'launching' 'launchpad' 'layer' 'lead' 'leadership' 'learn'\n",
      " 'learned' 'learning' 'leaving' 'led' 'left' 'length' 'level' 'leverage'\n",
      " 'life' 'lifetime' 'lightspeed' 'like' 'likely' 'little' 'live' 'logic'\n",
      " 'long' 'look' 'looking' 'losing' 'lot' 'luxembourg' 'machine' 'main'\n",
      " 'make' 'making' 'managed' 'management' 'many' 'market' 'markets' 'masses'\n",
      " 'may' 'mean' 'mediterranean' 'medium' 'meet' 'meo' 'milestones' 'million'\n",
      " 'mindful' 'mitigate' 'mix' 'model' 'modules' 'moment' 'money' 'months'\n",
      " 'more' 'move' 'mpower' 'much' 'multi' 'myself' 'nature' 'necessarily'\n",
      " 'need' 'needed' 'needs' 'negotiations' 'neighbor' 'network' 'networks'\n",
      " 'new' 'news' 'next' 'no' 'nobody' 'northern' 'not' 'now' 'o3b'\n",
      " 'objective' 'observed' 'october' 'of' 'off' 'offer' 'offering' 'officer'\n",
      " 'offs' 'often' 'on' 'once' 'one' 'oneweb' 'open' 'operate' 'operating'\n",
      " 'operational' 'operations' 'operator' 'opportunities' 'options' 'or'\n",
      " 'orbit' 'other' 'otherwise' 'our' 'ours' 'out' 'outlined' 'outstripped'\n",
      " 'over' 'own' 'pace' 'packed' 'paris' 'part' 'partly' 'partner'\n",
      " 'partnered' 'partners' 'partnerships' 'past' 'path' 'pay' 'payload'\n",
      " 'pays' 'performance' 'performs' 'period' 'phenomena' 'phenomenon' 'pinto'\n",
      " 'place' 'plan' 'plans' 'pleased' 'point' 'points' 'position' 'possible'\n",
      " 'potential' 'potentially' 'power' 'premium' 'presentation' 'president'\n",
      " 'press' 'prevent' 'prevents' 'previous' 'previously' 'price' 'pricing'\n",
      " 'private' 'probably' 'proceeds' 'produce' 'products' 'profitability'\n",
      " 'profitable' 'project' 'promote' 'promoted' 'proud' 'prove' 'provide'\n",
      " 'provided' 'provider' 'public' 'put' 'quick' 'quickly' 'quite' 'radio'\n",
      " 'rainhart' 'raise' 'raised' 're' 'reach' 'reached' 'reaching' 'ready'\n",
      " 'realizing' 'really' 'receipt' 'recent' 'recently' 'reconnaissance'\n",
      " 'record' 'regionally' 'release' 'remain' 'remains' 'remote' 'replace'\n",
      " 'replaced' 'replacements' 'replicate' 'requirements' 'requisite'\n",
      " 'resolved' 'resolving' 'responsibilities' 'result' 'retiring' 'reusable'\n",
      " 'revenue' 'right' 'rob' 'rocket' 'role' 'round' 'running' 'rushing'\n",
      " 'russia' 'ruy' 'said' 'sake' 'same' 'satcoms' 'satellite' 'satellites'\n",
      " 'say' 'saying' 'says' 'scale' 'scheduled' 'seamlessly' 'second'\n",
      " 'security' 'see' 'seen' 'segment' 'segments' 'selective' 'sell'\n",
      " 'sensible' 'sensing' 'sept' 'september' 'serafini' 'series' 'serious'\n",
      " 'service' 'services' 'ses' 'set' 'sheet' 'shelf' 'shielding' 'shift'\n",
      " 'ship' 'ships' 'short' 'should' 'side' 'sidelines' 'sign' 'signals'\n",
      " 'silver' 'since' 'single' 'six' 'sixth' 'slated' 'sleep' 'slot' 'small'\n",
      " 'smaller' 'so' 'software' 'some' 'something' 'sometimes' 'soon' 'source'\n",
      " 'sources' 'space' 'spacenews' 'specific' 'spectrum' 'sporadically' 'spot'\n",
      " 'starlink' 'state' 'staying' 'steve' 'still' 'stock' 'strategic'\n",
      " 'strategy' 'strong' 'stronghold' 'studies' 'success' 'such' 'sudden'\n",
      " 'summer' 'supply' 'support' 'surprise' 'surprised' 'surveillance'\n",
      " 'systems' 'tackling' 'tactical' 'take' 'talk' 'target' 'team' 'technical'\n",
      " 'technology' 'telesat' 'tenets' 'term' 'terms' 'than' 'that' 'the'\n",
      " 'their' 'them' 'then' 'there' 'they' 'thing' 'things' 'think' 'thinking'\n",
      " 'this' 'those' 'though' 'three' 'throwing' 'time' 'timeline' 'timing'\n",
      " 'to' 'together' 'tomorrow' 'tools' 'toronto' 'towards' 'tracking' 'tried'\n",
      " 'trip' 'tripping' 'turns' 'two' 'ukraine' 'ultimately' 'under'\n",
      " 'understand' 'understanding' 'unfortunate' 'unique' 'unit' 'university'\n",
      " 'up' 'upcoming' 'updated' 'updates' 'upgraded' 'us' 'users' 'valuable'\n",
      " 've' 'verge' 'versa' 'very' 'viability' 'viasat' 'vice' 'views'\n",
      " 'virginia' 'want' 'was' 'wasn' 'way' 'we' 'week' 'weeks' 'well' 'were'\n",
      " 'what' 'when' 'where' 'whether' 'which' 'who' 'will' 'willing' 'window'\n",
      " 'wish' 'with' 'without' 'won' 'words' 'work' 'working' 'world' 'worlds'\n",
      " 'would' 'year' 'years' 'yet' 'you' 'your' 'zealand']\n",
      "[[ 0  0  2 ...  2  0  1]\n",
      " [ 2  1  1 ... 14  2  0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(1, 2):\n",
      "Feature Names: ['10' '10 years' '11' ... 'your take' 'zealand' 'zealand the']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 1 1]\n",
      " [2 2 1 ... 1 0 0]]\n",
      "Number of tokens: 2819\n",
      "==================================================\n",
      "ngram_range=(2, 2):\n",
      "Feature Names: ['10 years' '11 ka' '12 announcement' ... 'your leadership' 'your take'\n",
      " 'zealand the']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [2 1 1 ... 1 1 0]]\n",
      "Number of tokens: 2040\n",
      "==================================================\n",
      "ngram_range=(2, 3):\n",
      "Feature Names: ['10 years' '10 years but' '10 years of' ... 'your take on' 'zealand the'\n",
      " 'zealand the company']\n",
      "Tokens:\n",
      "[[0 0 0 ... 0 1 1]\n",
      " [2 1 1 ... 1 0 0]]\n",
      "Number of tokens: 4420\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer with different n-gram parameters\n",
    "for ngram_range in [(1, 2), (2, 2), (2, 3)]:\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    print(f\"ngram_range={ngram_range}:\")\n",
    "    print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "    print(\"Tokens:\")\n",
    "    print(X.toarray())\n",
    "    print(\"Number of tokens:\", X.toarray().shape[1])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with min_df=1: 779\n",
      "Number of tokens with min_df=2: 151\n",
      "Number of tokens with max_df=1: 628\n",
      "Number of tokens with max_df=2: 779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize CountVectorizer with min_df=1\n",
    "vectorizer_min_df_1 = CountVectorizer(min_df=1)\n",
    "X_min_df_1 = vectorizer_min_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with min_df=2\n",
    "vectorizer_min_df_2 = CountVectorizer(min_df=2)\n",
    "X_min_df_2 = vectorizer_min_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with min_df=1\n",
    "num_tokens_min_df_1 = X_min_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with min_df=2\n",
    "num_tokens_min_df_2 = X_min_df_2.shape[1]\n",
    "\n",
    "# Initialize CountVectorizer with max_df=1\n",
    "vectorizer_max_df_1 = CountVectorizer(max_df=1)\n",
    "X_max_df_1 = vectorizer_max_df_1.fit_transform(docs)\n",
    "\n",
    "# Initialize CountVectorizer with max_df=2\n",
    "vectorizer_max_df_2 = CountVectorizer(max_df=2)\n",
    "X_max_df_2 = vectorizer_max_df_2.fit_transform(docs)\n",
    "\n",
    "# Number of tokens with max_df=1\n",
    "num_tokens_max_df_1 = X_max_df_1.shape[1]\n",
    "\n",
    "# Number of tokens with max_df=2\n",
    "num_tokens_max_df_2 = X_max_df_2.shape[1]\n",
    "\n",
    "print(f\"Number of tokens with min_df=1: {num_tokens_min_df_1}\")\n",
    "print(f\"Number of tokens with min_df=2: {num_tokens_min_df_2}\")\n",
    "print(f\"Number of tokens with max_df=1: {num_tokens_max_df_1}\")\n",
    "print(f\"Number of tokens with max_df=2: {num_tokens_max_df_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12' '20' 'about' 'ago' 'all' 'also' 'although' 'an' 'and' 'another'\n",
      " 'applications' 'are' 'as' 'at' 'be' 'been' 'being' 'both' 'business'\n",
      " 'but' 'by' 'can' 'capabilities' 'capital' 'chief' 'company' 'conference'\n",
      " 'conservative' 'continue' 'could' 'customers' 'data' 'demand' 'didn'\n",
      " 'different' 'do' 'doesn' 'felt' 'for' 'frequency' 'from' 'future' 'get'\n",
      " 'go' 'going' 'good' 'great' 'has' 'have' 'he' 'him' 'if' 'in' 'initial'\n",
      " 'interview' 'into' 'is' 'issue' 'it' 'its' 'just' 'last' 'later' 'launch'\n",
      " 'leverage' 'like' 'lot' 'make' 'market' 'may' 'mix' 'money' 'months'\n",
      " 'more' 'move' 'much' 'need' 'next' 'not' 'now' 'of' 'officer' 'on' 'one'\n",
      " 'or' 'orbit' 'other' 'our' 'out' 'over' 'paris' 'plan' 'plans' 'point'\n",
      " 'position' 'possible' 'potentially' 'products' 'put' 're' 'reached'\n",
      " 'right' 'rushing' 'said' 'satellite' 'satellites' 'service' 'since'\n",
      " 'smaller' 'so' 'some' 'still' 'such' 'tenets' 'term' 'than' 'that' 'the'\n",
      " 'their' 'them' 'then' 'there' 'they' 'think' 'this' 'those' 'three'\n",
      " 'throwing' 'time' 'to' 'together' 'two' 'understanding' 'up' 'us' 'very'\n",
      " 'was' 'we' 'week' 'well' 'what' 'where' 'which' 'will' 'with' 'work'\n",
      " 'world' 'would' 'year' 'years' 'you']\n",
      "[[ 2.  1.  1.  2.  1.  2.  1.  6. 22.  1.  2.  3. 13.  3.  3.  1.  1.  3.\n",
      "   2.  2.  3.  5.  3.  1.  3. 14.  1.  1.  1.  1.  3.  4.  1.  1.  1.  3.\n",
      "   1.  1.  9.  2.  3.  1.  1.  1.  3.  1.  1.  7.  3. 13.  1.  1. 15.  1.\n",
      "   1.  3.  7.  1.  6.  2.  1.  1.  1.  2.  1.  3.  1.  1.  2.  1.  1.  2.\n",
      "   1.  2.  1.  1.  2.  1.  2.  1. 19.  1.  7.  4.  3.  1.  1.  3.  1.  1.\n",
      "   1.  1.  2.  2.  2.  1.  1.  1.  1.  3.  1.  2.  1. 15.  2.  5.  1.  2.\n",
      "   1.  1.  1.  1.  2.  1.  1.  1. 13. 47.  1.  1.  1.  1.  1.  1.  1.  3.\n",
      "   3.  1.  2. 25.  1.  3.  1.  1.  2.  2.  2.  9.  1.  3.  1.  1.  4.  5.\n",
      "   4.  5.  1.  2.  3.  3.  2.]\n",
      " [ 1.  1.  3.  1.  2.  7.  1.  5. 53.  1.  1. 15. 13.  8. 10.  9.  4.  2.\n",
      "   2. 14.  4.  6.  1.  1.  1.  4.  1.  1.  1.  3.  7.  1.  3.  2.  1.  4.\n",
      "   5.  1. 25.  1. 11.  1.  2.  3.  6.  7.  1.  6. 29.  3.  2.  4. 47.  2.\n",
      "   1.  1. 23.  2. 21.  4.  4.  1.  1.  5.  1.  4.  2.  2.  6.  2.  1.  2.\n",
      "   4.  6.  1.  1.  3.  2.  9.  2. 35.  1. 20.  1.  6.  5.  2. 15.  2.  4.\n",
      "   1.  1.  1.  1.  3.  1.  1.  1.  2.  9.  1.  2.  1.  2.  2. 10.  2.  1.\n",
      "   1.  5.  4.  2.  1.  1.  1.  3. 31. 85.  3.  6.  1. 11.  8.  1.  8.  2.\n",
      "   1.  1. 10. 57.  1.  6.  2.  3.  7.  6. 10. 56.  1.  1. 10.  3.  4. 10.\n",
      "  21.  1.  1.  2.  1.  7. 14.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None,min_df=2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Token 'the' has the largest TF-IDF value of 47.00\n",
      "Document 2: Token 'the' has the largest TF-IDF value of 85.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Find the index of the token with the highest TF-IDF value in each document\n",
    "max_tfidf_indices = tfidf_matrix.argmax(axis=1)\n",
    "\n",
    "# Get the token with the largest TF-IDF value in each document\n",
    "for doc_index, max_index in enumerate(max_tfidf_indices):\n",
    "    token = feature_names[max_index]\n",
    "    tfidf_value = tfidf_matrix[doc_index, max_index]\n",
    "    print(f\"Document {doc_index + 1}: Token '{token}' has the largest TF-IDF value of {tfidf_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "WASHINGTON â As India prepares to launch its second lunar lander mission, the fate of a second Israeli lander is in doubt after the organization developing it lost a major source of funding. Indiaâs Chandrayaan-3 spacecraft is scheduled to launch July 14 on a Geosynchronous Satellite Launch Vehicle \n",
      "match 286:\n",
      "HELSINKI â India will make its second moon landing attempt in 18 daysâ time after its Chandrayaan-3 spacecraft arrived in lunar orbit Saturday. Chandrayaan-3 began a roughly 30-minute burn around 9:30 a.m. Eastern, seeing the spacecraft enter an elliptical lunar orbit, the Indian Space Research Orga\n",
      "match 700:\n",
      "HELSINKI â Indiaâs Chandrayaan-3 lander successfully touched down on the moon Wednesday, making the country only the fourth to achieve the feat. The Chandrayaan-3 mission lander touched down in the vicinity of the lunar South Pole region at 8:32 a.m. Eastern (1232 UTC) Aug. 23 after a 19-minute powe\n",
      "match 450:\n",
      "WASHINGTON â The technical success of Indiaâs Chandrayaan-3 lunar lander mission could help not just Indiaâs space program but also the countryâs standing on the global stage, experts argue. The Indian space agency ISRO  put the Vikram lander into sleep mode late Sept. 3 , shortly before nightfall a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "docs = pd.read_csv(\"spacenews.csv\")['content']\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.2)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "indices = np.arange(docs.size)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(indices, test_size=0.2)\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=3,metric=cosine_distances).fit(X[train])\n",
    "test=[test[0]]\n",
    "found = nbrs.kneighbors(X[test], return_distance=False)\n",
    "test_i=0\n",
    "print('text:\\n%.300s'%docs[test[test_i]])\n",
    "for i in found[0]:\n",
    "    print('match %d:\\n%.300s'%(i,docs[train[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
